{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "UalpOX0JkPq4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import albumentations as albu\n",
        "import numpy as np\n",
        "import gc\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import jaccard_score, precision_score, recall_score, accuracy_score, f1_score\n",
        "import glob\n",
        "from PIL import Image\n",
        "from skimage.io import imread\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from keras.layers import BatchNormalization, add\n",
        "from keras.layers import Conv2D, SeparableConv2D, UpSampling2D\n",
        "from keras.layers import add\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T06:44:39.098191Z",
          "iopub.status.busy": "2024-01-10T06:44:39.097017Z",
          "iopub.status.idle": "2024-01-10T06:44:39.469091Z",
          "shell.execute_reply": "2024-01-10T06:44:39.468066Z",
          "shell.execute_reply.started": "2024-01-10T06:44:39.098154Z"
        },
        "id": "LS8fsXn3kPq5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjMsHYebBoMO"
      },
      "outputs": [],
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.set_visible_devices(physical_devices[0], \"GPU\")\n",
        "for dev in physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(dev, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T06:44:40.873527Z",
          "iopub.status.busy": "2024-01-10T06:44:40.873168Z",
          "iopub.status.idle": "2024-01-10T06:44:40.87865Z",
          "shell.execute_reply": "2024-01-10T06:44:40.877619Z",
          "shell.execute_reply.started": "2024-01-10T06:44:40.873499Z"
        },
        "id": "VZwxyE9ckPq5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "os.makedirs('ProgressFull', exist_ok=True)\n",
        "os.makedirs(f'ModelSaveTensorFlow', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T06:44:43.694368Z",
          "iopub.status.busy": "2024-01-10T06:44:43.693978Z",
          "iopub.status.idle": "2024-01-10T06:44:44.266142Z",
          "shell.execute_reply": "2024-01-10T06:44:44.265367Z",
          "shell.execute_reply.started": "2024-01-10T06:44:43.694338Z"
        },
        "id": "5si2qaSJkPq5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "img_size = 352\n",
        "dataset_type = 'Kvasir-SEG/'\n",
        "folder_path = \"Kvasir-SEG/\"\n",
        "learning_rate = 1e-4\n",
        "seed_value = 58800\n",
        "filters = 17\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "ct = datetime.now()\n",
        "\n",
        "model_type = \"DuckNet\"\n",
        "\n",
        "progress_path = os.path.join('ProgressFull', dataset_type + '_progress_csv_' + model_type + '_filters_' + str(filters) +  '_' + str(ct) + '.csv')\n",
        "progressfull_path = os.path.join('ProgressFull', dataset_type + '_progress_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.txt')\n",
        "plot_path = os.path.join('ProgressFull', dataset_type + '_progress_plot_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.png')\n",
        "model_path = 'ModelSaveTensorFlow' + dataset_type + '/' + model_type + '_filters_' + str(filters) + '_' + str(ct)\n",
        "\n",
        "EPOCHS = 600\n",
        "min_loss_for_saving = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T06:44:46.43388Z",
          "iopub.status.busy": "2024-01-10T06:44:46.433519Z",
          "iopub.status.idle": "2024-01-10T06:44:46.638423Z",
          "shell.execute_reply": "2024-01-10T06:44:46.637436Z",
          "shell.execute_reply.started": "2024-01-10T06:44:46.43385Z"
        },
        "id": "nuB836J7kPq5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_data(img_height, img_width, images_to_be_loaded, dataset):\n",
        "    IMAGES_PATH = folder_path + 'images/'\n",
        "    MASKS_PATH = folder_path + 'masks/'\n",
        "    train_ids = glob.glob(IMAGES_PATH + \"*.jpg\")\n",
        "\n",
        "    if images_to_be_loaded == -1:\n",
        "        images_to_be_loaded = len(train_ids)\n",
        "\n",
        "    X_train = np.zeros((images_to_be_loaded, img_height, img_width, 3), dtype=np.float32)\n",
        "    Y_train = np.zeros((images_to_be_loaded, img_height, img_width), dtype=np.uint8)\n",
        "\n",
        "    print('Resizing training images and masks: ' + str(images_to_be_loaded))\n",
        "    for n, id_ in tqdm(enumerate(train_ids)):\n",
        "        if n == images_to_be_loaded:\n",
        "            break\n",
        "\n",
        "        image_path = id_\n",
        "        mask_path = image_path.replace(\"images\", \"masks\")\n",
        "\n",
        "        image = imread(image_path)\n",
        "        mask_ = imread(mask_path)\n",
        "\n",
        "        mask = np.zeros((img_height, img_width), dtype=np.bool_)\n",
        "\n",
        "        pillow_image = Image.fromarray(image)\n",
        "\n",
        "        pillow_image = pillow_image.resize((img_height, img_width))\n",
        "        image = np.array(pillow_image)\n",
        "\n",
        "        X_train[n] = image / 255\n",
        "\n",
        "        pillow_mask = Image.fromarray(mask_)\n",
        "        pillow_mask = pillow_mask.resize((img_height, img_width), resample=Image.LANCZOS)\n",
        "        mask_ = np.array(pillow_mask)\n",
        "\n",
        "        for i in range(img_height):\n",
        "            for j in range(img_width):\n",
        "                if (mask_[i, j] >= 127).any():\n",
        "                    mask[i, j] = 1\n",
        "\n",
        "        Y_train[n] = mask\n",
        "\n",
        "    Y_train = np.expand_dims(Y_train, axis=-1)\n",
        "\n",
        "    return X_train, Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T06:44:50.050322Z",
          "iopub.status.busy": "2024-01-10T06:44:50.049719Z",
          "iopub.status.idle": "2024-01-10T06:58:18.469263Z",
          "shell.execute_reply": "2024-01-10T06:58:18.468288Z",
          "shell.execute_reply.started": "2024-01-10T06:44:50.050286Z"
        },
        "id": "4FLaov6HkPq6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "X, Y = load_data(img_size, img_size, -1, 'kvasir')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T07:02:56.55026Z",
          "iopub.status.busy": "2024-01-10T07:02:56.549855Z",
          "iopub.status.idle": "2024-01-10T07:02:57.424941Z",
          "shell.execute_reply": "2024-01-10T07:02:57.424129Z",
          "shell.execute_reply.started": "2024-01-10T07:02:56.550229Z"
        },
        "id": "H0S1s_DXkPq6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, shuffle= True, random_state = seed_value)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.111, shuffle= True, random_state = seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T07:02:58.306933Z",
          "iopub.status.busy": "2024-01-10T07:02:58.306007Z",
          "iopub.status.idle": "2024-01-10T07:02:58.315037Z",
          "shell.execute_reply": "2024-01-10T07:02:58.314176Z",
          "shell.execute_reply.started": "2024-01-10T07:02:58.306901Z"
        },
        "id": "NgyyaQMBkPq6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "aug_train = albu.Compose([\n",
        "    albu.HorizontalFlip(),\n",
        "    albu.VerticalFlip(),\n",
        "    albu.ColorJitter(brightness=(0.6,1.6), contrast=0.2, saturation=0.1, hue=0.01, always_apply=True),\n",
        "    albu.Affine(scale=(0.5,1.5), translate_percent=(-0.125,0.125), rotate=(-180,180), shear=(-22.5,22), always_apply=True),\n",
        "])\n",
        "\n",
        "def augment_images():\n",
        "    x_train_out = []\n",
        "    y_train_out = []\n",
        "\n",
        "    for i in range (len(x_train)):\n",
        "        ug = aug_train(image=x_train[i], mask=y_train[i])\n",
        "        x_train_out.append(ug['image'])\n",
        "        y_train_out.append(ug['mask'])\n",
        "\n",
        "    return np.array(x_train_out), np.array(y_train_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T07:03:01.939441Z",
          "iopub.status.busy": "2024-01-10T07:03:01.938758Z",
          "iopub.status.idle": "2024-01-10T07:03:01.945454Z",
          "shell.execute_reply": "2024-01-10T07:03:01.944429Z",
          "shell.execute_reply.started": "2024-01-10T07:03:01.939409Z"
        },
        "id": "ZjFZ1JTIkPq6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def dice_metric_loss(ground_truth, predictions, smooth=1e-6):\n",
        "    ground_truth = tf.cast(ground_truth, tf.float32)\n",
        "    predictions = tf.cast(predictions, tf.float32)\n",
        "\n",
        "    ground_truth = Flatten()(ground_truth)\n",
        "    predictions = Flatten()(predictions)\n",
        "\n",
        "    intersection = tf.reduce_sum(predictions * ground_truth)\n",
        "    union = tf.reduce_sum(predictions) + tf.reduce_sum(ground_truth)\n",
        "\n",
        "    dice = (2. * intersection + smooth) / (union + smooth)\n",
        "\n",
        "    return 1 - dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T07:03:05.332236Z",
          "iopub.status.busy": "2024-01-10T07:03:05.331862Z",
          "iopub.status.idle": "2024-01-10T07:03:05.354412Z",
          "shell.execute_reply": "2024-01-10T07:03:05.353523Z",
          "shell.execute_reply.started": "2024-01-10T07:03:05.332206Z"
        },
        "id": "MMmT--OekPq7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "kernel_initializer = 'he_uniform'\n",
        "\n",
        "\n",
        "def conv_block_2D(x, filters, block_type, repeat=1, dilation_rate=1, size=3, padding='same'):\n",
        "    result = x\n",
        "\n",
        "    for i in range(0, repeat):\n",
        "\n",
        "        if block_type == 'separated':\n",
        "            result = separated_conv2D_block(result, filters, size=size, padding=padding)\n",
        "        elif block_type == 'duckv2':\n",
        "            result = duckv2_conv2D_block(result, filters, size=size)\n",
        "        elif block_type == 'midscope':\n",
        "            result = midscope_conv2D_block(result, filters)\n",
        "        elif block_type == 'widescope':\n",
        "            result = widescope_conv2D_block(result, filters)\n",
        "        elif block_type == 'resnet':\n",
        "            result = resnet_conv2D_block(result, filters, dilation_rate)\n",
        "        elif block_type == 'conv':\n",
        "            result = Conv2D(filters, (size, size),\n",
        "                            activation='relu', kernel_initializer=kernel_initializer, padding=padding)(result)\n",
        "\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def duckv2_conv2D_block(x, filters, size):\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x1 = widescope_conv2D_block(x, filters)\n",
        "\n",
        "    x2 = midscope_conv2D_block(x, filters)\n",
        "\n",
        "    x3 = conv_block_2D(x, filters, 'resnet', repeat=1)\n",
        "\n",
        "    x4 = conv_block_2D(x, filters, 'resnet', repeat=2)\n",
        "\n",
        "    x5 = conv_block_2D(x, filters, 'resnet', repeat=3)\n",
        "\n",
        "    x6 = separated_conv2D_block(x, filters, size=6, padding='same')\n",
        "\n",
        "    x = add([x1, x2, x3, x4, x5, x6])\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def separated_conv2D_block(x, filters, size=3, padding='same'):\n",
        "    x = Conv2D(filters, (1, size), activation='relu', kernel_initializer=kernel_initializer, padding=padding)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    x = SeparableConv2D(filters, (size, 1), activation='relu', depthwise_initializer=kernel_initializer, pointwise_initializer=kernel_initializer, padding=padding)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def midscope_conv2D_block(x, filters):\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=1)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    x = SeparableConv2D(filters, (3, 3), activation='relu', depthwise_initializer=kernel_initializer, pointwise_initializer=kernel_initializer, padding='same', dilation_rate=2)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def widescope_conv2D_block(x, filters):\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=1)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "\n",
        "    x = SeparableConv2D(filters, (3, 3), activation='relu', depthwise_initializer=kernel_initializer, pointwise_initializer=kernel_initializer, padding='same', dilation_rate=2)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "\n",
        "    x = SeparableConv2D(filters, (3, 3), activation='relu', depthwise_initializer=kernel_initializer, pointwise_initializer=kernel_initializer, padding='same', dilation_rate=3)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_conv2D_block(x, filters, dilation_rate=1):\n",
        "    x1 = Conv2D(filters, (1, 1), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "                dilation_rate=dilation_rate)(x)\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=dilation_rate)(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same',\n",
        "               dilation_rate=dilation_rate)(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x_final = add([x, x1])\n",
        "\n",
        "    x_final = BatchNormalization(axis=-1)(x_final)\n",
        "\n",
        "    return x_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T07:03:10.911973Z",
          "iopub.status.busy": "2024-01-10T07:03:10.911058Z",
          "iopub.status.idle": "2024-01-10T07:03:10.932061Z",
          "shell.execute_reply": "2024-01-10T07:03:10.930916Z",
          "shell.execute_reply.started": "2024-01-10T07:03:10.911936Z"
        },
        "id": "CD5JnO1DkPq7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "kernel_initializer = 'he_uniform'\n",
        "interpolation = \"nearest\"\n",
        "\n",
        "\n",
        "def create_model(img_height, img_width, input_chanels, out_classes, starting_filters):\n",
        "    input_layer = tf.keras.layers.Input((img_height, img_width, input_chanels))\n",
        "\n",
        "    print('Starting DUCK-Net')\n",
        "\n",
        "    p1 = Conv2D(starting_filters * 2, 2, strides=2, padding='same')(input_layer)\n",
        "    p2 = Conv2D(starting_filters * 4, 2, strides=2, padding='same')(p1)\n",
        "    p3 = Conv2D(starting_filters * 8, 2, strides=2, padding='same')(p2)\n",
        "    p4 = Conv2D(starting_filters * 16, 2, strides=2, padding='same')(p3)\n",
        "    p5 = Conv2D(starting_filters * 32, 2, strides=2, padding='same')(p4)\n",
        "\n",
        "    t0 = conv_block_2D(input_layer, starting_filters, 'duckv2', repeat=1)\n",
        "\n",
        "    l1i = SeparableConv2D(starting_filters * 2, 2, 2, 'same')(t0)\n",
        "    s1 = add([l1i, p1])\n",
        "    t1 = conv_block_2D(s1, starting_filters * 2, 'duckv2', repeat=1)\n",
        "\n",
        "    l2i = SeparableConv2D(starting_filters * 4, 2, 2, 'same')(t1)\n",
        "    s2 = add([l2i, p2])\n",
        "    t2 = conv_block_2D(s2, starting_filters * 4, 'duckv2', repeat=1)\n",
        "\n",
        "    l3i = SeparableConv2D(starting_filters * 8, 2, 2, 'same')(t2)\n",
        "    s3 = add([l3i, p3])\n",
        "    t3 = conv_block_2D(s3, starting_filters * 8, 'duckv2', repeat=1)\n",
        "\n",
        "    l4i = SeparableConv2D(starting_filters * 16, 2, 2, 'same')(t3)\n",
        "    s4 = add([l4i, p4])\n",
        "    t4 = conv_block_2D(s4, starting_filters * 16, 'duckv2', repeat=1)\n",
        "\n",
        "    l5i = SeparableConv2D(starting_filters * 32, 2, 2, 'same')(t4)\n",
        "    s5 = add([l5i, p5])\n",
        "    t51 = conv_block_2D(s5, starting_filters * 32, 'resnet', repeat=2)\n",
        "    t53 = conv_block_2D(t51, starting_filters * 16, 'resnet', repeat=2)\n",
        "\n",
        "    l5o = UpSampling2D((2, 2), interpolation=interpolation)(t53)\n",
        "    c4 = add([l5o, t4])\n",
        "    q4 = conv_block_2D(c4, starting_filters * 8, 'duckv2', repeat=1)\n",
        "\n",
        "    l4o = UpSampling2D((2, 2), interpolation=interpolation)(q4)\n",
        "    c3 = add([l4o, t3])\n",
        "    q3 = conv_block_2D(c3, starting_filters * 4, 'duckv2', repeat=1)\n",
        "\n",
        "    l3o = UpSampling2D((2, 2), interpolation=interpolation)(q3)\n",
        "    c2 = add([l3o, t2])\n",
        "    q6 = conv_block_2D(c2, starting_filters * 2, 'duckv2', repeat=1)\n",
        "\n",
        "    l2o = UpSampling2D((2, 2), interpolation=interpolation)(q6)\n",
        "    c1 = add([l2o, t1])\n",
        "    q1 = conv_block_2D(c1, starting_filters, 'duckv2', repeat=1)\n",
        "\n",
        "    l1o = UpSampling2D((2, 2), interpolation=interpolation)(q1)\n",
        "    c0 = add([l1o, t0])\n",
        "    z1 = conv_block_2D(c0, starting_filters, 'duckv2', repeat=1)\n",
        "\n",
        "    output = Conv2D(out_classes, (1, 1), activation='sigmoid')(z1)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T07:03:13.216585Z",
          "iopub.status.busy": "2024-01-10T07:03:13.216224Z",
          "iopub.status.idle": "2024-01-10T07:03:19.497115Z",
          "shell.execute_reply": "2024-01-10T07:03:19.496111Z",
          "shell.execute_reply.started": "2024-01-10T07:03:13.216556Z"
        },
        "id": "ldeKjgJAkPq7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = create_model(img_height=img_size, img_width=img_size, input_chanels=3, out_classes=1, starting_filters=filters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T07:03:26.788776Z",
          "iopub.status.busy": "2024-01-10T07:03:26.787935Z",
          "iopub.status.idle": "2024-01-10T07:03:26.823491Z",
          "shell.execute_reply": "2024-01-10T07:03:26.822725Z",
          "shell.execute_reply.started": "2024-01-10T07:03:26.78874Z"
        },
        "id": "RAKApVFHkPq7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, loss=dice_metric_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T07:03:28.654138Z",
          "iopub.status.busy": "2024-01-10T07:03:28.653755Z"
        },
        "id": "eVxul54ikPq7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "step = 0\n",
        "\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "    print(f'Training, epoch {epoch}')\n",
        "    print('Learning Rate: ' + str(learning_rate))\n",
        "\n",
        "    step += 1\n",
        "\n",
        "    image_augmented, mask_augmented = augment_images()\n",
        "\n",
        "    model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=4, validation_data=(x_valid, y_valid), verbose=1, callbacks=[tensorboard_callback])\n",
        "\n",
        "    prediction_valid = model.predict(x_valid, verbose=0)\n",
        "    loss_valid = dice_metric_loss(y_valid, prediction_valid)\n",
        "\n",
        "    loss_valid = loss_valid.numpy()\n",
        "    print(\"Loss Validation: \" + str(loss_valid))\n",
        "\n",
        "    prediction_test = model.predict(x_test, verbose=0)\n",
        "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
        "    loss_test = loss_test.numpy()\n",
        "    print(\"Loss Test: \" + str(loss_test))\n",
        "\n",
        "\n",
        "    if min_loss_for_saving > loss_valid:\n",
        "        min_loss_for_saving = loss_valid\n",
        "        print(\"Saved model with val_loss: \", loss_valid)\n",
        "        model.save('ModelSaveTensorFlow/11_dice_loss_loss_sperable_cov_Rmsprops' + str(loss_valid) + '_epoch_' + str(epoch) + '.keras')\n",
        "\n",
        "    del image_augmented\n",
        "    del mask_augmented\n",
        "\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUQXUQAXkPq7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(\"Loading the model\")\n",
        "\n",
        "model = tf.keras.models.load_model(\"ModelSaveTensorFlow/11_dice_loss_loss_sperable_cov_Rmsprops0.08849794_epoch_438.keras\", custom_objects={'dice_metric_loss':dice_metric_loss})\n",
        "\n",
        "prediction_train = model.predict(x_train, batch_size=4)\n",
        "prediction_valid = model.predict(x_valid, batch_size=4)\n",
        "prediction_test = model.predict(x_test, batch_size=4)\n",
        "\n",
        "print(\"Predictions done\")\n",
        "\n",
        "dice_train = f1_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_train > 0.5))\n",
        "dice_test = f1_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                          np.ndarray.flatten(prediction_test > 0.5))\n",
        "dice_valid = f1_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Dice finished\")\n",
        "\n",
        "\n",
        "miou_train = jaccard_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_train > 0.5))\n",
        "miou_test = jaccard_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                          np.ndarray.flatten(prediction_test > 0.5))\n",
        "miou_valid = jaccard_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Miou finished\")\n",
        "\n",
        "\n",
        "precision_train = precision_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                                  np.ndarray.flatten(prediction_train > 0.5))\n",
        "precision_test = precision_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                                 np.ndarray.flatten(prediction_test > 0.5))\n",
        "precision_valid = precision_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                                  np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Precision finished\")\n",
        "\n",
        "recall_train = recall_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                            np.ndarray.flatten(prediction_train > 0.5))\n",
        "recall_test = recall_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_test > 0.5))\n",
        "recall_valid = recall_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                            np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Recall finished\")\n",
        "\n",
        "accuracy_train = accuracy_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                                np.ndarray.flatten(prediction_train > 0.5))\n",
        "accuracy_test = accuracy_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                               np.ndarray.flatten(prediction_test > 0.5))\n",
        "accuracy_valid = accuracy_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                                np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Accuracy finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mksZfgw7BoMR"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "print(\"Train Accuracy:\", accuracy_train)\n",
        "print(\"Test Accuracy:\", accuracy_test)\n",
        "print(\"Validation Accuracy:\", accuracy_valid)\n",
        "\n",
        "# Mean Intersection over Union (mIoU)\n",
        "print(\"Train mIoU:\", miou_train)\n",
        "print(\"Test mIoU:\", miou_test)\n",
        "print(\"Validation mIoU:\", miou_valid)\n",
        "\n",
        "# Recall\n",
        "print(\"Train Recall:\", recall_train)\n",
        "print(\"Test Recall:\", recall_test)\n",
        "print(\"Validation Recall:\", recall_valid)\n",
        "\n",
        "# Dice Coefficient\n",
        "print(\"Train Dice:\", dice_train)\n",
        "print(\"Test Dice:\", dice_test)\n",
        "print(\"Validation Dice:\", dice_valid)\n",
        "\n",
        "# Precision\n",
        "print(\"Train Precision:\", precision_train)\n",
        "print(\"Test Precision:\", precision_test)\n",
        "print(\"Validation Precision:\", precision_valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsSYt7Z6BoMR"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 3837073,
          "sourceId": 6647157,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}